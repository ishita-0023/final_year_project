# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XfcvIxWxd4v96poNdZbg2YAgW8XBVkOj
"""

import gensim.downloader as api

w2v_model = api.load('word2vec-google-news-300')

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

# Recreate your model's architecture
def construct_model():
    model = Sequential()
    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))
    model.add(LSTM(64, recurrent_dropout=0.4))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='relu'))
    return model

# Construct the model
model = construct_model()

# Load weights into the new model
model.load_weights('/content/lstm_model3.h5')

# Compile the model after loading weights
model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])

import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
from gensim.models import KeyedVectors

# Load the Word2Vec model


# Clean and preprocess the text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'$$.*?$$', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(f"[{re.escape(string.punctuation)}]", '', text)
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text

def clean_stopwords(text):
    words = word_tokenize(text)
    return [word for word in words if word not in stopwords.words('english') and word.isalpha()]

def get_feature_vector(words, model, num_features=300):
    feature_vector = np.zeros((num_features,), dtype='float32')
    num_words = 0.
    index2word_set = set(model.index_to_key)

    for word in words:
        if word in index2word_set:
            num_words += 1
            feature_vector = np.add(feature_vector, model[word])

    if num_words > 0:
        feature_vector = np.divide(feature_vector, num_words)

    return feature_vector

def preprocess_essay(essay_text):
    cleaned_essay = clean_text(essay_text)
    cleaned_words = clean_stopwords(cleaned_essay)
    return get_feature_vector(cleaned_words, w2v_model).reshape(1, 1, 300)

# Input your essay text here
essay_text = "i am ishita. i am good girl"

# Preprocess the essay
input_vector = preprocess_essay(essay_text)

# Make prediction
predicted_grade = model.predict(input_vector)
rounded_grade = np.around(predicted_grade)

print(f"The predicted grade for the essay is: {rounded_grade[0][0]}")

import nltk
nltk.download('punkt')
nltk.download('stopwords')

