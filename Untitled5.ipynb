{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6vfyAtNSTeI",
        "outputId": "180e85f6-352b-40bf-8dad-1242e63ed87e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Recreate your model's architecture\n",
        "def construct_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "    return model\n",
        "\n",
        "# Construct the model\n",
        "model = construct_model()\n",
        "\n",
        "# Load weights into the new model\n",
        "model.load_weights('/content/lstm_model3.h5')\n",
        "\n",
        "# Compile the model after loading weights\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7778yBWPWEQh",
        "outputId": "b2eeec90-04e9-4977-93aa-b9c2a98510e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the Word2Vec model\n",
        "\n",
        "\n",
        "# Clean and preprocess the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'$$.*?$$', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return [word for word in words if word not in stopwords.words('english') and word.isalpha()]\n",
        "\n",
        "def get_feature_vector(words, model, num_features=300):\n",
        "    feature_vector = np.zeros((num_features,), dtype='float32')\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.index_to_key)\n",
        "\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "\n",
        "    if num_words > 0:\n",
        "        feature_vector = np.divide(feature_vector, num_words)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "def preprocess_essay(essay_text):\n",
        "    cleaned_essay = clean_text(essay_text)\n",
        "    cleaned_words = clean_stopwords(cleaned_essay)\n",
        "    return get_feature_vector(cleaned_words, w2v_model).reshape(1, 1, 300)\n",
        "\n",
        "# Input your essay text here\n",
        "essay_text = \"i am ishita. i am good girl\"\n",
        "\n",
        "# Preprocess the essay\n",
        "input_vector = preprocess_essay(essay_text)\n",
        "\n",
        "# Make prediction\n",
        "predicted_grade = model.predict(input_vector)\n",
        "rounded_grade = np.around(predicted_grade)\n",
        "\n",
        "print(f\"The predicted grade for the essay is: {rounded_grade[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBqojsX4WJrt",
        "outputId": "2b68db08-d255-4cd6-eda7-73ee9dae2691"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "The predicted grade for the essay is: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9PW9ODUYImb",
        "outputId": "282a22a8-1a89-4ab5-c5e0-fe608e1b5f7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsvziPNcYaHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}